{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process PPI network\n",
    "\n",
    "This notebook serves a roughly identical function as the following two. This notebook exists to format the PPI network for feature computation and analysis. Specifically, the following tasks are performed:\n",
    "\n",
    "1. Download and save raw data into `../data/1.raw/`\n",
    "2. Convert the network relationships from two sources (high-throughput and low-throughput experimental results) into a single network, including mapping relationships to a common identifier.\n",
    "3. Save the edges that appear in one of three networks (`train`, `test_recon`, or `test_new`) into the file at `../data/2.edges/ppi.tsv.xz`.\n",
    "4. Process to have not just edges that appear in one network but all possible node pairs for nodes that have an edge in the training network. This is for prediction of edges. Save this in `../data/3.all_nodes/ppi.tsv.xz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path('../../data/')\n",
    "data_path.joinpath('1.raw/').mkdir(exist_ok=True, parents=True)\n",
    "data_path.joinpath('2.edges/').mkdir(exist_ok=True, parents=True)\n",
    "data_path.joinpath('3.all_nodes/').mkdir(exist_ok=True, parents=True)\n",
    "data_path.joinpath('4.data/').mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download raw files\n",
    "\n",
    "### Protein-protein interaction networks\n",
    "\n",
    "* STRING https://string-db.org/\n",
    "* High-throughput, systematic PPIs\n",
    "    * We use two networks from the same group, both created through high-throughput screening. Data is available for download at http://interactome.baderlab.org/download.\n",
    "    * Rual et al. (2005) *Nature* https://www.ncbi.nlm.nih.gov/pubmed/16189514\n",
    "    * Rolland et al. (2014) *Cell* https://www.ncbi.nlm.nih.gov/pubmed/25416956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_url = {\n",
    "    'ppi_string.txt.gz': ('https://stringdb-static.org/download/protein.links.v11.0/'\n",
    "                          '9606.protein.links.v11.0.txt.gz'),\n",
    "    \n",
    "    'ppi_string_mapping.tsv.gz': ('https://string-db.org/mapping_files/uniprot/'\n",
    "                                  'human.uniprot_2_string.2018.tsv.gz'),\n",
    "    \n",
    "    'ppi_ht_1.psi': 'http://interactome.baderlab.org/data/Raul-Vidal(Nature_2005).psi',\n",
    "    'ppi_ht_2.psi': 'http://interactome.baderlab.org/data/Rolland-Vidal(Cell_2014).psi',\n",
    "}\n",
    "\n",
    "# for file, url in file_to_url.items():\n",
    "#     with open(data_path.joinpath(f'1.raw/{file}'), 'wb') as f:\n",
    "#         res = requests.get(url)\n",
    "#         f.write(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process files to edges\n",
    "\n",
    "Processing is generally as follows: \n",
    "\n",
    "(Note this example is for an undirected network with self-loops)\n",
    "\n",
    "1. Convert raw relationship data (in whatever form) to the following (excluding all duplicates):\n",
    "\n",
    "| source \t| target \t| network A \t| network B \t|\n",
    "|--------\t|--------\t|-----------\t|-----------\t|\n",
    "| A      \t| B      \t| 1         \t| 0         \t|\n",
    "| A      \t| C      \t| 1         \t| 1         \t|\n",
    "| B      \t| C      \t| 0         \t| 1         \t|\n",
    "\n",
    "2. Assign 70% of Network1 edges to the training network. Map the nodes in the training network to the integers 0, ..., num(nodes)-1. If the network is undirected, ensure that `id_a` $\\leq$ `id_b`. If the network is directed, index the source nodes first, (0, ..., num(source)-1), then target nodes (num(source),...). This mapping is done for the convenience of XSwap later. Results in `[network name]_edges_df`, which have the following schema:\n",
    "\n",
    "| source \t| target \t| source_id \t| target_id \t| train \t| network A \t| network B \t|\n",
    "|--------\t|--------\t|-----------\t|-----------\t|-------\t|-----------\t|-----------\t|\n",
    "| A      \t| B      \t| 0         \t| 1         \t| 0     \t| 1         \t| 0         \t|\n",
    "| A      \t| C      \t| 0         \t| 2         \t| 1     \t| 1         \t| 1         \t|\n",
    "| B      \t| C      \t| 1         \t| 2         \t| 0     \t| 0         \t| 1         \t|\n",
    "\n",
    "3. Take the subset of nodes that have an edge in the training network. The Cartesian product of these nodes will be the `[network_name]_df`, which have the following schema:\n",
    "\n",
    "| source \t| target \t| source_id \t| target_id \t| train \t| network A \t| network B \t|\n",
    "|--------\t|--------\t|-----------\t|-----------\t|-------\t|-----------\t|-----------\t|\n",
    "| A      \t| A      \t| 0         \t| 0         \t| 0     \t| 0         \t| 0         \t|\n",
    "| A      \t| B      \t| 0         \t| 1         \t| 0     \t| 1         \t| 0         \t|\n",
    "| A      \t| C      \t| 0         \t| 2         \t| 1     \t| 1         \t| 1         \t|\n",
    "| B      \t| B      \t| 1         \t| 1         \t| 0     \t| 0         \t| 0         \t|\n",
    "| B      \t| C      \t| 1         \t| 2         \t| 0     \t| 0         \t| 1         \t|\n",
    "| C      \t| C      \t| 2         \t| 2         \t| 0     \t| 0         \t| 0         \t|\n",
    "\n",
    "\n",
    "## 2.1 PPI\n",
    "\n",
    "### 2.1.1 STRING\n",
    "\n",
    "The two PPI networks use different mappings. We convert STRING to UniProt identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_a</th>\n",
       "      <th>uniprot_b</th>\n",
       "      <th>test_recon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A024R161</td>\n",
       "      <td>A0A075B734</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A024R161</td>\n",
       "      <td>A2A3L6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uniprot_a   uniprot_b  test_recon\n",
       "0  A0A024R161  A0A075B734           1\n",
       "1  A0A024R161      A2A3L6           1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensembl to UniProtKB identifier mappings\n",
    "mapping_df = pd.read_csv(data_path.joinpath('1.raw/ppi_string_mapping.tsv.gz'), sep='\\t',\n",
    "                         compression='gzip', names=['species', 'uniprot_entry', 'string', \n",
    "                                                      'unknown_a', 'unknown_b'])\n",
    "\n",
    "# Create dictionary with mappings\n",
    "string_to_uniprot = (\n",
    "    mapping_df\n",
    "    .assign(uniprot=lambda df: df['uniprot_entry'].apply(lambda x: re.search('[A-Z0-9]+', x).group()))\n",
    "    .set_index('string')\n",
    "    .loc[:, 'uniprot']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Load PPI network from STRING\n",
    "string_edges = set(map(tuple, map(sorted,\n",
    "    pd.read_csv(data_path.joinpath('1.raw/ppi_string.txt.gz'), compression='gzip', \n",
    "                sep=' ', dtype=str)\n",
    "    .assign(\n",
    "        uniprot_a=lambda df: df['protein1'].map(string_to_uniprot),\n",
    "        uniprot_b=lambda df: df['protein2'].map(string_to_uniprot),\n",
    "    )\n",
    "    .dropna()\n",
    "    .loc[:, ['uniprot_a', 'uniprot_b']]\n",
    "    .values\n",
    ")))\n",
    "\n",
    "string_edges_df = (\n",
    "    pd.DataFrame(sorted(string_edges), columns=['uniprot_a', 'uniprot_b'])\n",
    "    .assign(\n",
    "        test_recon=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "string_edges_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 High-throughput PPI network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_a</th>\n",
       "      <th>uniprot_b</th>\n",
       "      <th>test_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A024R0Y4</td>\n",
       "      <td>A0A0R4J2E4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A024R0Y4</td>\n",
       "      <td>O14964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uniprot_a   uniprot_b  test_new\n",
       "0  A0A024R0Y4  A0A0R4J2E4         1\n",
       "1  A0A024R0Y4      O14964         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the two networks\n",
    "ht_df = pd.concat([\n",
    "    pd.read_csv(data_path.joinpath('1.raw/ppi_ht_1.psi'), sep='\\t'), \n",
    "    pd.read_csv(data_path.joinpath('1.raw/ppi_ht_2.psi'), sep='\\t')\n",
    "], ignore_index=True)\n",
    "\n",
    "ht_edges = set(map(tuple, map(sorted, \n",
    "    ht_df\n",
    "    .rename(columns={\n",
    "        'Unique identifier for interactor A': 'ida', \n",
    "        'Unique identifier for interactor B': 'idb'})\n",
    "    .filter(items=['ida', 'idb',])\n",
    "    .query('ida != \"-\" and idb != \"-\"')\n",
    "    .assign(\n",
    "        uniprot_a=lambda df: df['ida'].apply(lambda x: re.search('(?<=uniprotkb:)[0-9A-Z]+', x).group()),\n",
    "        uniprot_b=lambda df: df['idb'].apply(lambda x: re.search('(?<=uniprotkb:)[0-9A-Z]+', x).group()),\n",
    "    )\n",
    "    .loc[:, ['uniprot_a', 'uniprot_b']]\n",
    "    .values\n",
    ")))\n",
    "\n",
    "ht_edges_df = (\n",
    "    pd.DataFrame(sorted(ht_edges), columns=['uniprot_a', 'uniprot_b'])\n",
    "    .assign(\n",
    "        test_new=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "ht_edges_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Combined PPI network\n",
    "\n",
    "Now, having two PPI networks both mapped to UniProt identifiers, we subset to the intersection of the two sets of nodes, using only nodes that are present in both networks. Then we map the shared nodes to IDs, unique integers from 0 to the number of shared nodes. This is done for efficiency in XSwap later on. Finally, as the edges are undirected, they are sorted so that the first ID is always <= the second ID. This ensures that we don't accidentally miss duplicates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRING: 19080 nodes\n",
      "HT: 4517 nodes\n",
      "SHARED: 4083 nodes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>id_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>train</th>\n",
       "      <th>test_recon</th>\n",
       "      <th>test_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A087WT00</td>\n",
       "      <td>O00154</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A087WT00</td>\n",
       "      <td>O43736</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name_a  name_b  id_a  id_b  train  test_recon  test_new\n",
       "0  A0A087WT00  O00154     0    48      1           1         0\n",
       "1  A0A087WT00  O43736     0   237      0           1         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only use nodes that are present in both networks\n",
    "string_nodes = set(string_edges_df.loc[:, 'uniprot_a':'uniprot_b'].values.flatten())\n",
    "ht_nodes = set(ht_edges_df.loc[:, 'uniprot_a':'uniprot_b'].values.flatten())\n",
    "shared_nodes = set(string_nodes.intersection(ht_nodes))\n",
    "\n",
    "print(f'STRING: {len(string_nodes)} nodes\\nHT: {len(ht_nodes)} nodes\\n'\n",
    "      f'SHARED: {len(shared_nodes)} nodes')\n",
    "\n",
    "# Join DataFrames and subset to node pairs consisting only of nodes shared between both networks\n",
    "np.random.seed(0)\n",
    "ppi_edges_df = (\n",
    "    string_edges_df  # ERROR COULD BE IN OUTER JOIN\n",
    "    .merge(ht_edges_df, how='outer', on=['uniprot_a', 'uniprot_b'])\n",
    "    .loc[lambda df: (df['uniprot_a'].apply(lambda x: x in shared_nodes) & \n",
    "                     df['uniprot_b'].apply(lambda x: x in shared_nodes))]\n",
    "    .fillna(0)\n",
    "    .assign(\n",
    "        train=lambda df: df['test_recon'].apply(lambda x: x == 1 and np.random.rand() < 0.7).astype(int),\n",
    "        test_recon=lambda df: df['test_recon'].astype(int),\n",
    "        test_new=lambda df: df['test_new'].astype(int),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Map nodes onto unique integers (for XSwap)\n",
    "ppi_nodes = sorted(set(\n",
    "    ppi_edges_df\n",
    "    .query('train == 1')\n",
    "    .loc[:, 'uniprot_a':'uniprot_b']\n",
    "    .values.flatten()\n",
    "))\n",
    "ppi_mapping = {name: i for name, i in zip(ppi_nodes, range(len(ppi_nodes)))}\n",
    "ppi_reversed_mapping = {v: k for k, v in ppi_mapping.items()}\n",
    "\n",
    "# Create a DF of all edges whose nodes have an edge in at least one of the networks\n",
    "ppi_edges_df = (\n",
    "    ppi_edges_df\n",
    "    .assign(\n",
    "        mapped_a=lambda df: df['uniprot_a'].map(ppi_mapping),\n",
    "        mapped_b=lambda df: df['uniprot_b'].map(ppi_mapping),\n",
    "    )\n",
    "    # Drop node pairs with nodes not in the train network\n",
    "    .dropna()\n",
    "    .assign(\n",
    "        # Edges are bi-directional, so make id_a <= id_b\n",
    "        id_a=lambda df: df.apply(lambda row: min(row['mapped_a'], row['mapped_b']), axis=1).astype(int),\n",
    "        id_b=lambda df: df.apply(lambda row: max(row['mapped_a'], row['mapped_b']), axis=1).astype(int),\n",
    "        \n",
    "        # Re-ordering means that UniProt IDs may now be reversed. \n",
    "        # Apply reverse mapping to ensure correctness.\n",
    "        name_a=lambda df: df['id_a'].map(ppi_reversed_mapping),\n",
    "        name_b=lambda df: df['id_b'].map(ppi_reversed_mapping),\n",
    "    )\n",
    "    .filter(items=['name_a', 'name_b', 'id_a', 'id_b', 'train', 'test_recon', 'test_new'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "ppi_edges_df.to_csv(data_path.joinpath('2.edges/ppi.tsv.xz'), compression='xz', index=False, sep='\\t')\n",
    "\n",
    "ppi_edges_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 53s, sys: 3.13 s, total: 3min 56s\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ppi_df = analysis.process_edges_to_full_network(ppi_edges_df, ppi_mapping, allow_loop=True, directed=False)\n",
    "ppi_df.to_csv(data_path.joinpath('3.all_nodes/ppi.tsv.xz'), compression='xz', index=False, sep='\\t')\n",
    "\n",
    "ppi_df.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xswap-analysis] *",
   "language": "python",
   "name": "conda-env-xswap-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
