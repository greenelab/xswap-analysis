{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import tqdm\n",
    "\n",
    "original = set(dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_summary_metrics(df):\n",
    "    nrow = len(df)\n",
    "    \n",
    "    # Save test edges\n",
    "    y_true = df['edge'].values\n",
    "    \n",
    "    # Compute features and hold in memory as numpy arrays (less memory use than pandas column)\n",
    "    degree_product = df['source_degree'].values * df['target_degree'].values\n",
    "    analytic_prior = degree_product / (degree_product - df['source_degree'].values\n",
    "                                       - df['target_degree'].values + df['edge'].sum() + 1)\n",
    "    del df['source_degree'], df['target_degree']\n",
    "    \n",
    "    metrics = list()\n",
    "    for feature in ['xswap_prior', 'analytic_prior', 'scaled_degree']:        \n",
    "        if feature == 'analytic_prior': \n",
    "            df['analytic_prior'] = analytic_prior\n",
    "            del analytic_prior\n",
    "        elif feature == 'scaled_degree':\n",
    "            df['scaled_degree'] = degree_product / degree_product.max()\n",
    "            del degree_product\n",
    "            \n",
    "        # Compute fraction and number of duplicates for each feature value\n",
    "        row = (\n",
    "            df\n",
    "            .groupby(feature)\n",
    "            .agg({'edge': ['count', 'mean']})\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        # Scored edges for later AUROC computation. (So df[feature] can be deleted ASAP)\n",
    "        y_score = df[feature].values\n",
    "        del df[feature]\n",
    "        \n",
    "        # Reformat from Dict[Tuple[str, str]: pd.Series] to Dict[str: np.ndarray]\n",
    "        row = {\n",
    "            feature: row[(feature, '')].values,\n",
    "            'count': row[('edge', 'count')].values,\n",
    "            'fraction_edges': row[('edge', 'mean')].values,\n",
    "        }\n",
    "\n",
    "        # Summarize feature values into single statistics\n",
    "        row = {\n",
    "            'feature': feature,\n",
    "            'metaedge': metaedge,\n",
    "            'cal': (row['count'] * (row[feature] - row['fraction_edges']) ** 2).sum() / nrow,\n",
    "            'ref': (row['count'] * row['fraction_edges'] * (1 - row['fraction_edges'])).sum() / nrow,\n",
    "            'auroc': sklearn.metrics.roc_auc_score(y_true, y_score),\n",
    "        }\n",
    "        row['brier'] = row['cal'] + row['ref']\n",
    "        metrics.append(row)\n",
    "        del y_score, row\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_network_all_metrics = list()\n",
    "sampled_network_all_metrics = list()\n",
    "\n",
    "full_prior_path = pathlib.Path('full_priors/')\n",
    "sampled_prior_path = pathlib.Path('sampled_priors/')\n",
    "prior_paths = sorted(list(full_prior_path.glob('*.tsv.gz')))\n",
    "# prior_paths = [pathlib.Path('full_priors/G<rG.tsv.gz'), ]\n",
    "# prior_paths = [pathlib.Path('full_priors/AlD.tsv.gz'), ]\n",
    "\n",
    "for prior_path in tqdm.tqdm_notebook(prior_paths):\n",
    "    metaedge = prior_path.name.split('.')[0]\n",
    "    print(metaedge, flush=True)\n",
    "    \n",
    "    # Load DataFrame and produce dataframe with features and outcomes only (save memory)\n",
    "    prior_df = pd.read_csv(prior_path, sep='\\t', usecols=['edge', 'source_degree',\n",
    "                                                          'target_degree', 'xswap_prior'])\n",
    "    \n",
    "    full_network_all_metrics.extend(dataframe_to_summary_metrics(prior_df))\n",
    "    original_edges = prior_df['edge'].values.astype(bool)\n",
    "    del prior_df\n",
    "    \n",
    "    sampled_df = (\n",
    "        pd.read_csv(f'sampled_priors/{metaedge}.tsv.gz', sep='\\t', \n",
    "                    usecols=['edge', 'source_degree', 'target_degree', 'xswap_prior'])\n",
    "        .assign(original_edges=original_edges)\n",
    "        .query('edge == 0')\n",
    "        .drop('edge', axis=1)\n",
    "        .rename(columns={'original_edges': 'edge'})\n",
    "    )\n",
    "    del original_edges\n",
    "    sampled_network_all_metrics.extend(dataframe_to_summary_metrics(sampled_df))\n",
    "    del sampled_df['edge'], sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third task: Translating between degree sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_task_all_metrics = list()\n",
    "\n",
    "full_prior_path = pathlib.Path('../../data/4.data/')\n",
    "prior_paths = sorted(list(full_prior_path.glob('*.tsv.xz')))\n",
    "# prior_paths = [pathlib.Path('full_priors/G<rG.tsv.xz'), ]\n",
    "\n",
    "for prior_path in tqdm.tqdm_notebook(prior_paths):\n",
    "    metaedge = prior_path.with_suffix('').stem\n",
    "    print(metaedge, flush=True)\n",
    "\n",
    "    prior_df = (\n",
    "        pd.read_csv(prior_path, sep='\\t', usecols=['id_a', 'id_b', \n",
    "                                                   'network', 'edge', 'edge_prior'])\n",
    "        .query('network != \"train\"')\n",
    "        .rename(columns={'edge_prior': 'xswap_prior'})\n",
    "    )\n",
    "\n",
    "    prior_df = (\n",
    "        prior_df\n",
    "        .query('network == \"test_recon\"')\n",
    "        .reset_index(drop=True)\n",
    "        .assign(\n",
    "            edge_other = prior_df.query('network == \"test_new\"')['edge'].values,\n",
    "        )\n",
    "        .drop(['network'], axis=1)\n",
    "        .assign(\n",
    "            source_degree = lambda df: df.groupby('id_a').transform(sum)['edge'],\n",
    "            target_degree = lambda df: df.groupby('id_b').transform(sum)['edge'],\n",
    "        )\n",
    "        .drop(['id_a', 'id_b', 'edge'], axis=1)\n",
    "        .rename(columns={'edge_other': 'edge'})\n",
    "\n",
    "    )\n",
    "    third_task_all_metrics.extend(dataframe_to_summary_metrics(prior_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metrics = pd.DataFrame.from_records(full_network_all_metrics)\n",
    "sampled_metrics = pd.DataFrame.from_records(sampled_network_all_metrics)\n",
    "third_task_metrics = pd.DataFrame.from_records(third_task_all_metrics)\n",
    "\n",
    "all_metrics = pd.concat([\n",
    "    full_metrics.assign(network='full'),\n",
    "    sampled_metrics.assign(network='sampled'),\n",
    "    third_task_metrics.assign(network='other')\n",
    "])\n",
    "\n",
    "all_metrics.to_csv('hetionet_calibration_metrics.csv', index=False, columns=['network', 'metaedge', 'feature',\n",
    "                                                                             'cal', 'ref', 'brier', 'auroc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xswap-analysis] *",
   "language": "python",
   "name": "conda-env-xswap-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
