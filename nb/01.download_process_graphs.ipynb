{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path('../data/1.raw/')\n",
    "data_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "file_to_url = {\n",
    "    'ppi_string.txt.gz': ('https://stringdb-static.org/download/protein.links.v11.0/'\n",
    "                          '9606.protein.links.v11.0.txt.gz'),\n",
    "    \n",
    "    'ppi_string_mapping.tsv.gz': ('https://string-db.org/mapping_files/uniprot/'\n",
    "                                  'human.uniprot_2_string.2018.tsv.gz'),\n",
    "    \n",
    "    'ppi_ht_1.psi': 'http://interactome.baderlab.org/data/Raul-Vidal(Nature_2005).psi',\n",
    "    'ppi_ht_2.psi': 'http://interactome.baderlab.org/data/Rolland-Vidal(Cell_2014).psi',\n",
    "    \n",
    "    'tftg.gmt': ('https://static-content.springer.com/esm/art%3A10.1186%2Fs12915-017-0469-0/'\n",
    "                 'MediaObjects/12915_2017_469_MOESM5_ESM.gmt')\n",
    "}\n",
    "\n",
    "# for file, url in file_to_url.items():\n",
    "#     with open(data_path.joinpath(file), 'wb') as f:\n",
    "#         res = requests.get(url)\n",
    "#         f.write(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process files to edges\n",
    "\n",
    "Processing is generally as follows: \n",
    "\n",
    "(Note this example is for an undirected network with self-loops)\n",
    "\n",
    "1. Convert raw relationship data (in whatever form) to \n",
    "\n",
    "| source \t| target \t| network A \t| network B \t|\n",
    "|--------\t|--------\t|-----------\t|-----------\t|\n",
    "| A      \t| B      \t| 1         \t| 0         \t|\n",
    "| A      \t| C      \t| 1         \t| 1         \t|\n",
    "| B      \t| C      \t| 0         \t| 1         \t|\n",
    "\n",
    "2. Assign 70% of Network1 edges to the training network. Map the nodes in the training network to the integers 0, ..., num(nodes)-1. If the network is undirected, ensure that `id_a` $\\leq$ `id_b`. If the network is directed, index the source nodes first, (0, ..., num(source)-1), then target nodes (num(source),...). This mapping is done for the convenience of XSwap later. Results in `[network name]_edges_df`, which have the following schema:\n",
    "\n",
    "| source \t| target \t| source_id \t| target_id \t| train \t| network A \t| network B \t|\n",
    "|--------\t|--------\t|-----------\t|-----------\t|-------\t|-----------\t|-----------\t|\n",
    "| A      \t| B      \t| 0         \t| 1         \t| 0     \t| 1         \t| 0         \t|\n",
    "| A      \t| C      \t| 0         \t| 2         \t| 1     \t| 1         \t| 1         \t|\n",
    "| B      \t| C      \t| 1         \t| 2         \t| 0     \t| 0         \t| 1         \t|\n",
    "\n",
    "3. Take the subset of nodes that have an edge in the training network. The Cartesian product of these nodes will be the `[network_name]_df`, which have the following schema:\n",
    "\n",
    "| source \t| target \t| source_id \t| target_id \t| train \t| network A \t| network B \t|\n",
    "|--------\t|--------\t|-----------\t|-----------\t|-------\t|-----------\t|-----------\t|\n",
    "| A      \t| A      \t| 0         \t| 0         \t| 0     \t| 0         \t| 0         \t|\n",
    "| A      \t| B      \t| 0         \t| 1         \t| 0     \t| 1         \t| 0         \t|\n",
    "| A      \t| C      \t| 0         \t| 2         \t| 1     \t| 1         \t| 1         \t|\n",
    "| B      \t| B      \t| 1         \t| 1         \t| 0     \t| 0         \t| 0         \t|\n",
    "| B      \t| C      \t| 1         \t| 2         \t| 0     \t| 0         \t| 1         \t|\n",
    "| C      \t| C      \t| 2         \t| 2         \t| 0     \t| 0         \t| 0         \t|\n",
    "\n",
    "\n",
    "## 2.1 PPI\n",
    "\n",
    "### 2.1.1 STRING\n",
    "\n",
    "https://string-db.org/\n",
    "\n",
    "The two PPI networks use different mappings. We convert STRING to UniProt identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.385 percent of edges had a node that could not be mapped to UniProt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_a</th>\n",
       "      <th>uniprot_b</th>\n",
       "      <th>test_recon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P84085</td>\n",
       "      <td>O43307</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P84085</td>\n",
       "      <td>O75460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniprot_a uniprot_b  test_recon\n",
       "1    P84085    O43307           1\n",
       "2    P84085    O75460           1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensembl to UniProtKB identifier mappings\n",
    "mapping_df = pd.read_table(data_path.joinpath('ppi_string_mapping.tsv.gz'), \n",
    "                           compression='gzip', names=['species', 'uniprot_entry', 'string', \n",
    "                                                      'unknown_a', 'unknown_b'])\n",
    "\n",
    "# Create dictionary with mappings\n",
    "string_to_uniprot = (\n",
    "    mapping_df\n",
    "    .assign(uniprot=lambda df: df['uniprot_entry'].apply(lambda x: re.search('[A-Z0-9]+', x).group()))\n",
    "    .set_index('string')\n",
    "    .loc[:, 'uniprot']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Load PPI network from STRING\n",
    "string_edges_df = (\n",
    "    pd.read_table(data_path.joinpath('ppi_string.txt.gz'), compression='gzip', sep=' ')\n",
    "    .assign(\n",
    "        uniprot_a=lambda df: df['protein1'].map(string_to_uniprot),\n",
    "        uniprot_b=lambda df: df['protein2'].map(string_to_uniprot),\n",
    "        test_recon=1,\n",
    "    )\n",
    "    .filter(items=['uniprot_a', 'uniprot_b', 'test_recon'])\n",
    ")\n",
    "\n",
    "# Some STRING identifiers cannot be mapped to UniProt. These appear as NA in string_df\n",
    "percent_unmapped = 100 * (string_edges_df.shape[0] - string_edges_df.dropna().shape[0]) \\\n",
    "                   / string_edges_df.shape[0]\n",
    "\n",
    "string_edges_df = string_edges_df.dropna()\n",
    "\n",
    "print(f'{percent_unmapped :.3f} percent of edges had a node that could not be mapped to UniProt')\n",
    "\n",
    "string_edges_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 High-throughput PPI network\n",
    "\n",
    "We use two networks from the same group, both created through high-throughput screening. Data is available for download at http://interactome.baderlab.org/download.\n",
    "\n",
    "Rual et al. (2005) *Nature* https://www.ncbi.nlm.nih.gov/pubmed/16189514\n",
    "\n",
    "Rolland et al. (2014) *Cell* https://www.ncbi.nlm.nih.gov/pubmed/25416956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_a</th>\n",
       "      <th>uniprot_b</th>\n",
       "      <th>test_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>O14964</td>\n",
       "      <td>A0A024R0Y4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>O95990</td>\n",
       "      <td>A0A024R0Y4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uniprot_a   uniprot_b  test_new\n",
       "142    O14964  A0A024R0Y4         1\n",
       "144    O95990  A0A024R0Y4         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the two networks\n",
    "ht_df = pd.concat([\n",
    "    pd.read_csv(data_path.joinpath('ppi_ht_1.psi'), sep='\\t'), \n",
    "    pd.read_csv(data_path.joinpath('ppi_ht_2.psi'), sep='\\t')\n",
    "], ignore_index=True)\n",
    "\n",
    "ht_edges_df = (\n",
    "    ht_df\n",
    "    .rename(columns={\n",
    "        'Unique identifier for interactor A': 'ida', \n",
    "        'Unique identifier for interactor B': 'idb'})\n",
    "    .filter(items=['ida', 'idb',])\n",
    "    .query('ida != \"-\" and idb != \"-\"')\n",
    "    .assign(\n",
    "        uniprot_a=lambda df: df['ida'].apply(lambda x: re.search('(?<=uniprotkb:)[0-9A-Z]+', x).group()),\n",
    "        uniprot_b=lambda df: df['idb'].apply(lambda x: re.search('(?<=uniprotkb:)[0-9A-Z]+', x).group()),\n",
    "        test_new=1,\n",
    "    )\n",
    "    .filter(items=['uniprot_a', 'uniprot_b', 'test_new'])\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "ht_edges_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Combined PPI network\n",
    "\n",
    "Now, having two PPI networks both mapped to UniProt identifiers, we subset to the intersection of the two sets of nodes, using only nodes that are present in both networks. Then we map the shared nodes to IDs, unique integers from 0 to the number of shared nodes. This is done for efficiency in XSwap later on. Finally, as the edges are undirected, they are sorted so that the first ID is always <= the second ID. This ensures that we don't accidentally miss duplicates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRING: 19080 nodes\n",
      "HT: 4517 nodes\n",
      "SHARED: 4083 nodes\n"
     ]
    }
   ],
   "source": [
    "# Only use nodes that are present in both networks\n",
    "string_nodes = set(string_edges_df.loc[:, 'uniprot_a':'uniprot_b'].values.flatten())\n",
    "ht_nodes = set(ht_edges_df.loc[:, 'uniprot_a':'uniprot_b'].values.flatten())\n",
    "shared_nodes = set(string_nodes.intersection(ht_nodes))\n",
    "\n",
    "print(f'STRING: {len(string_nodes)} nodes\\nHT: {len(ht_nodes)} nodes\\n'\n",
    "      f'SHARED: {len(shared_nodes)} nodes')\n",
    "\n",
    "# Join DataFrames and subset to node pairs consisting only of nodes shared between both networks\n",
    "np.random.seed(0)\n",
    "ppi_edges_df = (\n",
    "    string_edges_df\n",
    "    .merge(ht_edges_df, how='outer', on=['uniprot_a', 'uniprot_b'])\n",
    "    .loc[lambda df: (df['uniprot_a'].apply(lambda x: x in shared_nodes) & \n",
    "                     df['uniprot_b'].apply(lambda x: x in shared_nodes))]\n",
    "    .fillna(0)\n",
    "    .assign(\n",
    "        train=lambda df: df['test_recon'].apply(lambda x: x == 1 and np.random.rand() < 0.7).astype(int),\n",
    "        test_recon=lambda df: df['test_recon'].astype(int),\n",
    "        test_new=lambda df: df['test_new'].astype(int),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Map nodes onto unique integers (for XSwap)\n",
    "ppi_nodes = sorted(set(ppi_edges_df.loc[:, 'uniprot_a':'uniprot_b'].values.flatten()))\n",
    "ppi_mapping = {name: i for name, i in zip(ppi_nodes, range(len(ppi_nodes)))}\n",
    "ppi_reversed_mapping = {v: k for k, v in ppi_mapping.items()}\n",
    "\n",
    "# Create a DF of all edges whose nodes have an edge in at least one of the networks\n",
    "ppi_edges_df = (\n",
    "    ppi_edges_df\n",
    "    .assign(\n",
    "        mapped_a=lambda df: df['uniprot_a'].map(ppi_mapping),\n",
    "        mapped_b=lambda df: df['uniprot_b'].map(ppi_mapping),\n",
    "    )\n",
    "    # Drop node pairs with nodes not in the train network\n",
    "    .dropna()\n",
    "    .assign(\n",
    "        # Edges are bi-directional, so make id_a <= id_b\n",
    "        id_a=lambda df: df.apply(lambda row: min(row['mapped_a'], row['mapped_b']), axis=1).astype(int),\n",
    "        id_b=lambda df: df.apply(lambda row: max(row['mapped_a'], row['mapped_b']), axis=1).astype(int),\n",
    "        \n",
    "        # Re-ordering means that UniProt IDs may now be reversed. \n",
    "        # Apply reverse mapping to ensure correctness.\n",
    "        uniprot_a=lambda df: df['id_a'].map(ppi_reversed_mapping),\n",
    "        uniprot_b=lambda df: df['id_b'].map(ppi_reversed_mapping),\n",
    "    )\n",
    "    .filter(items=['uniprot_a', 'uniprot_b', 'id_a', 'id_b', 'train', 'test_recon', 'test_new'])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_a_x</th>\n",
       "      <th>uniprot_b_x</th>\n",
       "      <th>id_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>uniprot_a_y</th>\n",
       "      <th>uniprot_b_y</th>\n",
       "      <th>train</th>\n",
       "      <th>test_recon</th>\n",
       "      <th>test_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A087WT00</td>\n",
       "      <td>A0A087WT00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A087WT00</td>\n",
       "      <td>A0A0B4J1W7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniprot_a_x uniprot_b_x  id_a  id_b uniprot_a_y uniprot_b_y  train  \\\n",
       "0  A0A087WT00  A0A087WT00     0     0           0           0      0   \n",
       "1  A0A087WT00  A0A0B4J1W7     0     1           0           0      0   \n",
       "\n",
       "   test_recon  test_new  \n",
       "0           0         0  \n",
       "1           0         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the final DF of pairings for all nodes that appear in the training network\n",
    "uniprot_a, uniprot_b = zip(*itertools.product(ppi_nodes, ppi_nodes))\n",
    "\n",
    "ppi_df = (\n",
    "    pd.DataFrame()\n",
    "    .assign(\n",
    "        uniprot_a=uniprot_a,\n",
    "        uniprot_b=uniprot_b,\n",
    "        id_a=lambda df: df['uniprot_a'].map(ppi_mapping),\n",
    "        id_b=lambda df: df['uniprot_b'].map(ppi_mapping),\n",
    "    )\n",
    "    # Do not include duplicates of edges. \n",
    "    .query('id_a <= id_b')\n",
    "    # Merge with DF that already satisfies id_a <= id_b\n",
    "    .merge(ppi_edges_df, how='left', on=['id_a', 'id_b'])\n",
    "    .fillna(0)\n",
    "    .assign(\n",
    "        train=lambda df: df['train'].astype(int),\n",
    "        test_recon=lambda df: df['test_recon'].astype(int),\n",
    "        test_new=lambda df: df['test_new'].astype(int),\n",
    "    )\n",
    ")\n",
    "\n",
    "data_path.parent.joinpath('2.edges/').mkdir(exist_ok=True, parents=True)\n",
    "ppi_df.to_csv(data_path.parent.joinpath('2.edges/ppi.tsv.xz'), compression='xz',\n",
    "             index=False, sep='\\t')\n",
    "\n",
    "ppi_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the network is connected\n",
    "ppi_edges = list(map(tuple, ppi_df.query('train == 1').loc[:, 'id_a':'id_b'].values))\n",
    "G = nx.from_edgelist(ppi_edges)\n",
    "nx.is_connected(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 BioRxiv collaboration network\n",
    "\n",
    "Rxivist full database (doi:10.5281/zenodo.2566421) at https://zenodo.org/record/2566421\n",
    "\n",
    "\n",
    "I used the following query to export a copy of the Rxivist BioRxiv scrape.\n",
    "\n",
    "```sqlite\n",
    "SELECT \n",
    "    aa.article, \n",
    "    art.title,\n",
    "    auth.id as author_id, \n",
    "    auth.name as author_name, \n",
    "    auth.institution, \n",
    "    art.doi, \n",
    "    art.collection, \n",
    "    art.posted\n",
    "FROM prod.article_authors aa\n",
    "JOIN prod.authors auth\n",
    "\tON aa.author = auth.id\n",
    "JOIN prod.articles art\n",
    "\tON aa.article = art.id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Transcription factor - target gene (TFTG) network\n",
    "\n",
    "Data is originally in the form:\n",
    "\n",
    "Source: [target1, target2, ...]\n",
    "\n",
    "and needs to first be reformatted as an edge list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf_name</th>\n",
       "      <th>gene_name</th>\n",
       "      <th>test_recon</th>\n",
       "      <th>test_new</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AEBP2</td>\n",
       "      <td>AAGAB</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEBP2</td>\n",
       "      <td>ALDH4A1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tf_name gene_name  test_recon  test_new  train\n",
       "0   AEBP2     AAGAB           0         1      0\n",
       "1   AEBP2   ALDH4A1           0         1      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format data to edges\n",
    "tftg_records = list()\n",
    "with open(data_path.joinpath('tftg.gmt'), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        groups = line.strip().split('\\t')\n",
    "        tf_name, tf_entrez = groups[0].split('_')[-2:]\n",
    "        method = re.match('\\A([A-Za-z\\-\\ ]+)(?=(\\ Transcriptional|\\ TFTG))', groups[1]).group().lower()\n",
    "        for gene in groups[2:]:\n",
    "            tftg_records.append(\n",
    "                (tf_name, gene, method)\n",
    "            )\n",
    "\n",
    "# Format edges to a DataFrame. Randomly assign 70% of low-throughput edges to train, with\n",
    "# the withheld 30% being used for predicting reconstruction.\n",
    "np.random.seed(0)\n",
    "tftg_edges_df = (\n",
    "    pd.DataFrame\n",
    "    .from_records(tftg_records, columns=['tf_name', 'gene_name', 'method'])\n",
    "    .assign(\n",
    "        test_recon=lambda df: (df['method'] == 'low throughtput').astype(int),\n",
    "        test_new=lambda df: (df['method'] == 'chip-seq').astype(int),\n",
    "    )\n",
    "    .groupby(['tf_name', 'gene_name'])[['test_recon', 'test_new']].sum()\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        train=lambda df: df['test_recon'].apply(lambda x: x == 1 and np.random.rand() < 0.7).astype(int),\n",
    "    )\n",
    ")\n",
    "\n",
    "tftg_edges_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232 unique transcription factors\n",
      "14913 unique genes\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping between nodes and integers. Make TFs the lowest values, and non-TF genes later\n",
    "tfs = set(tftg_edges_df.query('train == 1').loc[:, 'tf_name'].values)\n",
    "genes = set(tftg_edges_df.query('train == 1').loc[:, 'gene_name'].values)\n",
    "genes_only = sorted(genes.difference(tfs))\n",
    "\n",
    "tfs = sorted(tfs)\n",
    "genes = sorted(genes)\n",
    "\n",
    "tf_mapping = {tf: i for i, tf in enumerate(tfs)}\n",
    "gene_mapping = {gene: len(tfs) + i for i, gene in enumerate(genes_only)}\n",
    "tftg_mapping = {**tf_mapping, **gene_mapping}\n",
    "\n",
    "print(\"{} unique transcription factors\\n{} unique genes\".format(len(tfs), len(genes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232 unique transcription factors\n",
      "14913 unique genes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf_name</th>\n",
       "      <th>gene_name</th>\n",
       "      <th>tf_id</th>\n",
       "      <th>gene_id</th>\n",
       "      <th>train</th>\n",
       "      <th>test_recon</th>\n",
       "      <th>test_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHR</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHR</td>\n",
       "      <td>A2M</td>\n",
       "      <td>0</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tf_name gene_name  tf_id  gene_id  train  test_recon  test_new\n",
       "0     AHR      A1CF      0      232      0           0         0\n",
       "1     AHR       A2M      0      233      0           0         0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_name, gene_name = zip(*itertools.product(tfs, genes))\n",
    "\n",
    "tftg_df = (\n",
    "    pd.DataFrame()\n",
    "    .assign(\n",
    "        tf_name=tf_name,\n",
    "        gene_name=gene_name,\n",
    "        tf_id=lambda df: df['tf_name'].map(tftg_mapping),\n",
    "        gene_id=lambda df: df['gene_name'].map(tftg_mapping),\n",
    "    )\n",
    "    .merge(tftg_edges_df, how='left', on=['tf_name', 'gene_name'])\n",
    "    .fillna(0)\n",
    "    .assign(\n",
    "        train=lambda df: df['train'].astype(int),\n",
    "        test_recon=lambda df: df['test_recon'].astype(int),\n",
    "        test_new=lambda df: df['test_new'].astype(int),\n",
    "    )\n",
    "    .filter(items=['tf_name', 'gene_name', 'tf_id', 'gene_id', 'train', 'test_recon', 'test_new'])\n",
    ")\n",
    "\n",
    "print(\"{} unique transcription factors\\n{} unique genes\".format(len(set(tftg_df['tf_name'])), \n",
    "                                                                len(set(tftg_df['gene_name']))))\n",
    "\n",
    "tftg_df.to_csv(data_path.parent.joinpath('2.edges/tftg.tsv.xz'), compression='xz',\n",
    "                     index=False, sep='\\t')\n",
    "\n",
    "tftg_df.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xswap-analysis]",
   "language": "python",
   "name": "conda-env-xswap-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
