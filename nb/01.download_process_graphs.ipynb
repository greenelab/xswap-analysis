{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_edges_to_full_network(edges_df, mapping, allow_loop=False, directed=False):\n",
    "    '''\n",
    "    Convert a DataFrame that contains only node pairs where an edge appears in one of \n",
    "    the three networks (train, test_recon, test_new) to a DataFrame of all node pairs \n",
    "    for nodes appearing in the test network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    edges_df : pandas.DataFrame\n",
    "        Columns should be ['name_a', 'name_b', 'id_a', 'id_b', 'train', 'test_recon', 'test_new'].\n",
    "        Contains only node pairs with at least one edge, ie. at minimum one of train, test_recon, \n",
    "        test_new is 1.\n",
    "    mapping : dict\n",
    "        Mapping from name to id\n",
    "    allow_loop : bool\n",
    "        Whether to include self-loops as potential edges.\n",
    "    directed : bool\n",
    "        Whether edge (a, b) is equivalent to (b, a). If directed, then only source-target node pairs\n",
    "        will be in the returned DataFrame\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "    '''\n",
    "    reversed_mapping = {v: k for k, v in mapping.items()}\n",
    "    train_df = edges_df.query('train == 1')\n",
    "    rel = '<=' if allow_loop else '<'\n",
    "    \n",
    "    if directed:\n",
    "        source_nodes = sorted(set(train_df['id_a']))\n",
    "        target_nodes = sorted(set(train_df['id_b']))\n",
    "        id_a, id_b = zip(*itertools.product(source_nodes, target_nodes))\n",
    "    else:\n",
    "        nodes = sorted(set(train_df[['id_a', 'id_b']].values.flatten()))\n",
    "        id_a, id_b = zip(*itertools.product(nodes, nodes))\n",
    "    \n",
    "    df = (\n",
    "        pd.DataFrame()\n",
    "        .assign(\n",
    "            id_a=id_a,\n",
    "            id_b=id_b,\n",
    "        )\n",
    "        .query(f'id_a {rel} id_b')\n",
    "        .merge(edges_df, how='left', on=['id_a', 'id_b'])\n",
    "        .assign(\n",
    "            name_a=lambda df: df['id_a'].map(reversed_mapping),\n",
    "            name_b=lambda df: df['id_b'].map(reversed_mapping),\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .assign(\n",
    "            train=lambda df: df['train'].astype(int),\n",
    "            test_recon=lambda df: df['test_recon'].astype(int),\n",
    "            test_new=lambda df: df['test_new'].astype(int),\n",
    "        )\n",
    "        .filter(items=['name_a', 'name_b', 'id_a', 'id_b', 'train', 'test_recon', 'test_new'])\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path('../data/')\n",
    "data_path.joinpath('1.raw/').mkdir(exist_ok=True, parents=True)\n",
    "data_path.joinpath('2.edges/').mkdir(exist_ok=True, parents=True)\n",
    "data_path.joinpath('3.all_nodes/').mkdir(exist_ok=True, parents=True)\n",
    "data_path.joinpath('4.data/').mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download raw files\n",
    "\n",
    "### Protein-protein interaction networks\n",
    "\n",
    "* STRING https://string-db.org/\n",
    "* High-throughput, systematic PPIs\n",
    "    * We use two networks from the same group, both created through high-throughput screening. Data is available for download at http://interactome.baderlab.org/download.\n",
    "    * Rual et al. (2005) *Nature* https://www.ncbi.nlm.nih.gov/pubmed/16189514\n",
    "    * Rolland et al. (2014) *Cell* https://www.ncbi.nlm.nih.gov/pubmed/25416956\n",
    "\n",
    "### BioRxiv collaboration network\n",
    "\n",
    "DOI: 10.1101/515643\n",
    "\n",
    "Rxivist full database (doi:10.5281/zenodo.2566421) at https://zenodo.org/record/2566421\n",
    "\n",
    "\n",
    "I used the following query to export a copy of the Rxivist BioRxiv scrape.\n",
    "\n",
    "```sqlite\n",
    "SELECT \n",
    "    aa.article, \n",
    "    art.title,\n",
    "    auth.id as author_id, \n",
    "    auth.name as author_name, \n",
    "    auth.institution, \n",
    "    art.doi, \n",
    "    art.collection, \n",
    "    art.posted\n",
    "FROM prod.article_authors aa\n",
    "JOIN prod.authors auth\n",
    "\tON aa.author = auth.id\n",
    "JOIN prod.articles art\n",
    "\tON aa.article = art.id\n",
    "```\n",
    "\n",
    "### Transcription-factor target-gene relationships\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/pubmed/29325558"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_url = {\n",
    "    'ppi_string.txt.gz': ('https://stringdb-static.org/download/protein.links.v11.0/'\n",
    "                          '9606.protein.links.v11.0.txt.gz'),\n",
    "    \n",
    "    'ppi_string_mapping.tsv.gz': ('https://string-db.org/mapping_files/uniprot/'\n",
    "                                  'human.uniprot_2_string.2018.tsv.gz'),\n",
    "    \n",
    "    'ppi_ht_1.psi': 'http://interactome.baderlab.org/data/Raul-Vidal(Nature_2005).psi',\n",
    "    'ppi_ht_2.psi': 'http://interactome.baderlab.org/data/Rolland-Vidal(Cell_2014).psi',\n",
    "    \n",
    "    'tftg.gmt': ('https://static-content.springer.com/esm/art%3A10.1186%2Fs12915-017-0469-0/'\n",
    "                 'MediaObjects/12915_2017_469_MOESM5_ESM.gmt')\n",
    "}\n",
    "\n",
    "# for file, url in file_to_url.items():\n",
    "#     with open(data_path.joinpath(f'1.raw/{file}'), 'wb') as f:\n",
    "#         res = requests.get(url)\n",
    "#         f.write(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process files to edges\n",
    "\n",
    "Processing is generally as follows: \n",
    "\n",
    "(Note this example is for an undirected network with self-loops)\n",
    "\n",
    "1. Convert raw relationship data (in whatever form) to \n",
    "\n",
    "| source \t| target \t| network A \t| network B \t|\n",
    "|--------\t|--------\t|-----------\t|-----------\t|\n",
    "| A      \t| B      \t| 1         \t| 0         \t|\n",
    "| A      \t| C      \t| 1         \t| 1         \t|\n",
    "| B      \t| C      \t| 0         \t| 1         \t|\n",
    "\n",
    "2. Assign 70% of Network1 edges to the training network. Map the nodes in the training network to the integers 0, ..., num(nodes)-1. If the network is undirected, ensure that `id_a` $\\leq$ `id_b`. If the network is directed, index the source nodes first, (0, ..., num(source)-1), then target nodes (num(source),...). This mapping is done for the convenience of XSwap later. Results in `[network name]_edges_df`, which have the following schema:\n",
    "\n",
    "| source \t| target \t| source_id \t| target_id \t| train \t| network A \t| network B \t|\n",
    "|--------\t|--------\t|-----------\t|-----------\t|-------\t|-----------\t|-----------\t|\n",
    "| A      \t| B      \t| 0         \t| 1         \t| 0     \t| 1         \t| 0         \t|\n",
    "| A      \t| C      \t| 0         \t| 2         \t| 1     \t| 1         \t| 1         \t|\n",
    "| B      \t| C      \t| 1         \t| 2         \t| 0     \t| 0         \t| 1         \t|\n",
    "\n",
    "3. Take the subset of nodes that have an edge in the training network. The Cartesian product of these nodes will be the `[network_name]_df`, which have the following schema:\n",
    "\n",
    "| source \t| target \t| source_id \t| target_id \t| train \t| network A \t| network B \t|\n",
    "|--------\t|--------\t|-----------\t|-----------\t|-------\t|-----------\t|-----------\t|\n",
    "| A      \t| A      \t| 0         \t| 0         \t| 0     \t| 0         \t| 0         \t|\n",
    "| A      \t| B      \t| 0         \t| 1         \t| 0     \t| 1         \t| 0         \t|\n",
    "| A      \t| C      \t| 0         \t| 2         \t| 1     \t| 1         \t| 1         \t|\n",
    "| B      \t| B      \t| 1         \t| 1         \t| 0     \t| 0         \t| 0         \t|\n",
    "| B      \t| C      \t| 1         \t| 2         \t| 0     \t| 0         \t| 1         \t|\n",
    "| C      \t| C      \t| 2         \t| 2         \t| 0     \t| 0         \t| 0         \t|\n",
    "\n",
    "\n",
    "## 2.1 PPI\n",
    "\n",
    "### 2.1.1 STRING\n",
    "\n",
    "The two PPI networks use different mappings. We convert STRING to UniProt identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_a</th>\n",
       "      <th>uniprot_b</th>\n",
       "      <th>test_recon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A024R161</td>\n",
       "      <td>A0A075B734</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A024R161</td>\n",
       "      <td>A2A3L6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uniprot_a   uniprot_b  test_recon\n",
       "0  A0A024R161  A0A075B734           1\n",
       "1  A0A024R161      A2A3L6           1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensembl to UniProtKB identifier mappings\n",
    "mapping_df = pd.read_table(data_path.joinpath('1.raw/ppi_string_mapping.tsv.gz'), \n",
    "                           compression='gzip', names=['species', 'uniprot_entry', 'string', \n",
    "                                                      'unknown_a', 'unknown_b'])\n",
    "\n",
    "# Create dictionary with mappings\n",
    "string_to_uniprot = (\n",
    "    mapping_df\n",
    "    .assign(uniprot=lambda df: df['uniprot_entry'].apply(lambda x: re.search('[A-Z0-9]+', x).group()))\n",
    "    .set_index('string')\n",
    "    .loc[:, 'uniprot']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Load PPI network from STRING\n",
    "string_edges = set(map(tuple, map(sorted,\n",
    "    pd.read_table(data_path.joinpath('1.raw/ppi_string.txt.gz'), compression='gzip', \n",
    "                  sep=' ', dtype=str)\n",
    "    .assign(\n",
    "        uniprot_a=lambda df: df['protein1'].map(string_to_uniprot),\n",
    "        uniprot_b=lambda df: df['protein2'].map(string_to_uniprot),\n",
    "    )\n",
    "    .dropna()\n",
    "    .loc[:, ['uniprot_a', 'uniprot_b']]\n",
    "    .values\n",
    ")))\n",
    "\n",
    "string_edges_df = (\n",
    "    pd.DataFrame(sorted(string_edges), columns=['uniprot_a', 'uniprot_b'])\n",
    "    .assign(\n",
    "        test_recon=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "string_edges_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 High-throughput PPI network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_a</th>\n",
       "      <th>uniprot_b</th>\n",
       "      <th>test_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A024R0Y4</td>\n",
       "      <td>A0A0R4J2E4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A024R0Y4</td>\n",
       "      <td>O14964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uniprot_a   uniprot_b  test_new\n",
       "0  A0A024R0Y4  A0A0R4J2E4         1\n",
       "1  A0A024R0Y4      O14964         1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the two networks\n",
    "ht_df = pd.concat([\n",
    "    pd.read_csv(data_path.joinpath('1.raw/ppi_ht_1.psi'), sep='\\t'), \n",
    "    pd.read_csv(data_path.joinpath('1.raw/ppi_ht_2.psi'), sep='\\t')\n",
    "], ignore_index=True)\n",
    "\n",
    "ht_edges = set(map(tuple, map(sorted, \n",
    "    ht_df\n",
    "    .rename(columns={\n",
    "        'Unique identifier for interactor A': 'ida', \n",
    "        'Unique identifier for interactor B': 'idb'})\n",
    "    .filter(items=['ida', 'idb',])\n",
    "    .query('ida != \"-\" and idb != \"-\"')\n",
    "    .assign(\n",
    "        uniprot_a=lambda df: df['ida'].apply(lambda x: re.search('(?<=uniprotkb:)[0-9A-Z]+', x).group()),\n",
    "        uniprot_b=lambda df: df['idb'].apply(lambda x: re.search('(?<=uniprotkb:)[0-9A-Z]+', x).group()),\n",
    "    )\n",
    "    .loc[:, ['uniprot_a', 'uniprot_b']]\n",
    "    .values\n",
    ")))\n",
    "\n",
    "ht_edges_df = (\n",
    "    pd.DataFrame(sorted(ht_edges), columns=['uniprot_a', 'uniprot_b'])\n",
    "    .assign(\n",
    "        test_new=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "ht_edges_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Combined PPI network\n",
    "\n",
    "Now, having two PPI networks both mapped to UniProt identifiers, we subset to the intersection of the two sets of nodes, using only nodes that are present in both networks. Then we map the shared nodes to IDs, unique integers from 0 to the number of shared nodes. This is done for efficiency in XSwap later on. Finally, as the edges are undirected, they are sorted so that the first ID is always <= the second ID. This ensures that we don't accidentally miss duplicates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRING: 19080 nodes\n",
      "HT: 4517 nodes\n",
      "SHARED: 4083 nodes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>id_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>train</th>\n",
       "      <th>test_recon</th>\n",
       "      <th>test_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A087WT00</td>\n",
       "      <td>O00154</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A087WT00</td>\n",
       "      <td>O43736</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name_a  name_b  id_a  id_b  train  test_recon  test_new\n",
       "0  A0A087WT00  O00154     0    48      1           1         0\n",
       "1  A0A087WT00  O43736     0   237      0           1         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only use nodes that are present in both networks\n",
    "string_nodes = set(string_edges_df.loc[:, 'uniprot_a':'uniprot_b'].values.flatten())\n",
    "ht_nodes = set(ht_edges_df.loc[:, 'uniprot_a':'uniprot_b'].values.flatten())\n",
    "shared_nodes = set(string_nodes.intersection(ht_nodes))\n",
    "\n",
    "print(f'STRING: {len(string_nodes)} nodes\\nHT: {len(ht_nodes)} nodes\\n'\n",
    "      f'SHARED: {len(shared_nodes)} nodes')\n",
    "\n",
    "# Join DataFrames and subset to node pairs consisting only of nodes shared between both networks\n",
    "np.random.seed(0)\n",
    "ppi_edges_df = (\n",
    "    string_edges_df  # ERROR COULD BE IN OUTER JOIN\n",
    "    .merge(ht_edges_df, how='outer', on=['uniprot_a', 'uniprot_b'])\n",
    "    .loc[lambda df: (df['uniprot_a'].apply(lambda x: x in shared_nodes) & \n",
    "                     df['uniprot_b'].apply(lambda x: x in shared_nodes))]\n",
    "    .fillna(0)\n",
    "    .assign(\n",
    "        train=lambda df: df['test_recon'].apply(lambda x: x == 1 and np.random.rand() < 0.7).astype(int),\n",
    "        test_recon=lambda df: df['test_recon'].astype(int),\n",
    "        test_new=lambda df: df['test_new'].astype(int),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Map nodes onto unique integers (for XSwap)\n",
    "ppi_nodes = sorted(set(\n",
    "    ppi_edges_df\n",
    "    .query('train == 1')\n",
    "    .loc[:, 'uniprot_a':'uniprot_b']\n",
    "    .values.flatten()\n",
    "))\n",
    "ppi_mapping = {name: i for name, i in zip(ppi_nodes, range(len(ppi_nodes)))}\n",
    "ppi_reversed_mapping = {v: k for k, v in ppi_mapping.items()}\n",
    "\n",
    "# Create a DF of all edges whose nodes have an edge in at least one of the networks\n",
    "ppi_edges_df = (\n",
    "    ppi_edges_df\n",
    "    .assign(\n",
    "        mapped_a=lambda df: df['uniprot_a'].map(ppi_mapping),\n",
    "        mapped_b=lambda df: df['uniprot_b'].map(ppi_mapping),\n",
    "    )\n",
    "    # Drop node pairs with nodes not in the train network\n",
    "    .dropna()\n",
    "    .assign(\n",
    "        # Edges are bi-directional, so make id_a <= id_b\n",
    "        id_a=lambda df: df.apply(lambda row: min(row['mapped_a'], row['mapped_b']), axis=1).astype(int),\n",
    "        id_b=lambda df: df.apply(lambda row: max(row['mapped_a'], row['mapped_b']), axis=1).astype(int),\n",
    "        \n",
    "        # Re-ordering means that UniProt IDs may now be reversed. \n",
    "        # Apply reverse mapping to ensure correctness.\n",
    "        name_a=lambda df: df['id_a'].map(ppi_reversed_mapping),\n",
    "        name_b=lambda df: df['id_b'].map(ppi_reversed_mapping),\n",
    "    )\n",
    "    .filter(items=['name_a', 'name_b', 'id_a', 'id_b', 'train', 'test_recon', 'test_new'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "ppi_edges_df.to_csv(data_path.joinpath('2.edges/ppi.tsv.xz'), compression='xz', index=False, sep='\\t')\n",
    "\n",
    "ppi_edges_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>id_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>train</th>\n",
       "      <th>test_recon</th>\n",
       "      <th>test_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A087WT00</td>\n",
       "      <td>A0A087WT00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A087WT00</td>\n",
       "      <td>A0A0B4J1W7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name_a      name_b  id_a  id_b  train  test_recon  test_new\n",
       "0  A0A087WT00  A0A087WT00     0     0      0           0         0\n",
       "1  A0A087WT00  A0A0B4J1W7     0     1      0           0         0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppi_df = process_edges_to_full_network(ppi_edges_df, ppi_mapping, allow_loop=True, directed=False)\n",
    "\n",
    "ppi_df.to_csv(data_path.joinpath('3.all_nodes/ppi.tsv.xz'), compression='xz', index=False, sep='\\t')\n",
    "\n",
    "ppi_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 BioRxiv collaboration network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>author_name</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>2146</td>\n",
       "      <td>Juri Rappsilber</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14115</th>\n",
       "      <td>2146</td>\n",
       "      <td>Swantje Lenz</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       article      author_name  year\n",
       "14114     2146  Juri Rappsilber  2018\n",
       "14115     2146     Swantje Lenz  2018"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rxivist_df = (\n",
    "    pd.read_csv('../data/1.raw/citations.csv.xz', compression='xz')\n",
    "    .dropna()\n",
    "    .assign(\n",
    "        year=lambda df: df['posted'].apply(lambda x: int(x[:4])),\n",
    "    )\n",
    "    .query('collection == \"bioinformatics\"')\n",
    "    .drop(columns=['title', 'author_id', 'institution', 'doi', 'posted', 'collection'])\n",
    ")\n",
    "\n",
    "rxivist_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>id_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>train</th>\n",
       "      <th>test_recon</th>\n",
       "      <th>test_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Juri Rappsilber</td>\n",
       "      <td>Lutz Fischer</td>\n",
       "      <td>3607</td>\n",
       "      <td>4175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kemal Eren</td>\n",
       "      <td>Venkatesh Kumar</td>\n",
       "      <td>3754</td>\n",
       "      <td>6944</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name_a           name_b  id_a  id_b  train  test_recon  test_new\n",
       "0  Juri Rappsilber     Lutz Fischer  3607  4175      0           0         1\n",
       "1       Kemal Eren  Venkatesh Kumar  3754  6944      0           0         1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cutoff = 2017\n",
    "\n",
    "# Self-join to create author-author relationships from author-article relationships\n",
    "biorxiv_edges_df = (\n",
    "    rxivist_df\n",
    "    .merge(rxivist_df, on=['article', 'year'], )\n",
    "    .rename(columns={'author_name_x': 'name_a', 'author_name_y': 'name_b'})\n",
    "    # Remove duplicate and self-edges\n",
    "    .query('name_a < name_b')\n",
    "    .assign(\n",
    "        test_recon=lambda df: df['year'].apply(lambda x: x <= train_cutoff).astype(int),\n",
    "        test_new=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Select articles for the training network\n",
    "articles = sorted(set(biorxiv_edges_df.query('test_recon == 1')['article']))\n",
    "train_articles = set(np.random.choice(articles, size=int(0.7*len(articles)), replace=False))\n",
    "biorxiv_edges_df = (\n",
    "    biorxiv_edges_df\n",
    "    .assign(train=lambda df: df['article'].apply(lambda x: x in train_articles).astype(int))\n",
    ")\n",
    "\n",
    "# Create a node mapping\n",
    "authors = sorted(set(biorxiv_edges_df.query('train == 1').loc[:, ['name_a', 'name_b']].values.flatten()))\n",
    "biorxiv_mapping = {name: i for name, i in zip(authors, range(len(authors)))}\n",
    "biorxiv_reversed = {v: k for k, v in biorxiv_mapping.items()}\n",
    "\n",
    "# Apply node mapping\n",
    "biorxiv_edges_df = (\n",
    "    biorxiv_edges_df\n",
    "    .assign(\n",
    "        mapped_a=lambda df: df['name_a'].map(biorxiv_mapping),\n",
    "        mapped_b=lambda df: df['name_b'].map(biorxiv_mapping),\n",
    "    )\n",
    "    .dropna()\n",
    "    .assign(\n",
    "        id_a=lambda df: df.apply(lambda row: min(row['mapped_a'], row['mapped_b']), axis=1).astype(int),\n",
    "        id_b=lambda df: df.apply(lambda row: max(row['mapped_a'], row['mapped_b']), axis=1).astype(int),\n",
    "        name_a=lambda df: df['id_a'].map(biorxiv_reversed),\n",
    "        name_b=lambda df: df['id_b'].map(biorxiv_reversed),\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    "    .filter(items=['name_a', 'name_b', 'id_a', 'id_b', 'train', 'test_recon', 'test_new'])\n",
    ")\n",
    "\n",
    "biorxiv_edges_df.to_csv(data_path.joinpath('2.edges/biorxiv.tsv.xz'), compression='xz', index=False, sep='\\t')\n",
    "\n",
    "biorxiv_edges_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>id_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>train</th>\n",
       "      <th>test_recon</th>\n",
       "      <th>test_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- The US-Venezuela Collaborative Research Group</td>\n",
       "      <td>A. Ercument Cicek</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>- The US-Venezuela Collaborative Research Group</td>\n",
       "      <td>A. S. M. Ashique Mahmood</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            name_a                    name_b  \\\n",
       "0  - The US-Venezuela Collaborative Research Group         A. Ercument Cicek   \n",
       "1  - The US-Venezuela Collaborative Research Group  A. S. M. Ashique Mahmood   \n",
       "\n",
       "   id_a  id_b  train  test_recon  test_new  \n",
       "0     0     1      0           0         0  \n",
       "1     0     2      0           0         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biorxiv_df = process_edges_to_full_network(biorxiv_edges_df, biorxiv_mapping, allow_loop=False, directed=False)\n",
    "\n",
    "biorxiv_df.to_csv(data_path.joinpath('3.all_nodes/biorxiv.tsv.xz'), compression='xz', index=False, sep='\\t')\n",
    "\n",
    "biorxiv_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Transcription factor - target gene (TFTG) network\n",
    "\n",
    "Data is originally in the form:\n",
    "\n",
    "Source: [target1, target2, ...]\n",
    "\n",
    "and needs to first be reformatted as an edge list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232 unique transcription factors\n",
      "14913 unique genes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>id_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>train</th>\n",
       "      <th>test_recon</th>\n",
       "      <th>test_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AEBP2</td>\n",
       "      <td>AAGAB</td>\n",
       "      <td>498</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEBP2</td>\n",
       "      <td>ALDH4A1</td>\n",
       "      <td>498</td>\n",
       "      <td>613</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name_a   name_b  id_a  id_b  train  test_recon  test_new\n",
       "0  AEBP2    AAGAB   498   239      0           0         1\n",
       "1  AEBP2  ALDH4A1   498   613      0           0         1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format data to edges\n",
    "tftg_records = list()\n",
    "with open(data_path.joinpath('1.raw/tftg.gmt'), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        groups = line.strip().split('\\t')\n",
    "        tf_name, tf_entrez = groups[0].split('_')[-2:]\n",
    "        method = re.match('\\A([A-Za-z\\-\\ ]+)(?=(\\ Transcriptional|\\ TFTG))', groups[1]).group().lower()\n",
    "        for gene in groups[2:]:\n",
    "            tftg_records.append(\n",
    "                (tf_name, gene, method)\n",
    "            )\n",
    "\n",
    "# Format edges to a DataFrame. Randomly assign 70% of low-throughput edges to train, with\n",
    "# the withheld 30% being used for predicting reconstruction.\n",
    "np.random.seed(0)\n",
    "tftg_edges_df = (\n",
    "    pd.DataFrame\n",
    "    .from_records(tftg_records, columns=['name_a', 'name_b', 'method'])\n",
    "    .assign(\n",
    "        test_recon=lambda df: (df['method'] == 'low throughtput').astype(int),\n",
    "        test_new=lambda df: (df['method'] == 'chip-seq').astype(int),\n",
    "    )\n",
    "    .groupby(['name_a', 'name_b'])[['test_recon', 'test_new']].sum()\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        train=lambda df: df['test_recon'].apply(lambda x: x == 1 and np.random.rand() < 0.7).astype(int),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a mapping between nodes and integers. Make TFs the lowest values, and non-TF genes later\n",
    "tfs = set(tftg_edges_df.query('train == 1').loc[:, 'name_a'].values)\n",
    "genes = set(tftg_edges_df.query('train == 1').loc[:, 'name_b'].values)\n",
    "genes_only = sorted(genes.difference(tfs))\n",
    "\n",
    "tfs = sorted(tfs)\n",
    "genes = sorted(genes)\n",
    "\n",
    "tf_mapping = {tf: i for i, tf in enumerate(tfs)}\n",
    "gene_mapping = {gene: len(tfs) + i for i, gene in enumerate(genes_only)}\n",
    "tftg_mapping = {**tf_mapping, **gene_mapping}\n",
    "\n",
    "print(\"{} unique transcription factors\\n{} unique genes\".format(len(tfs), len(genes)))\n",
    "\n",
    "tftg_edges_df = (\n",
    "    tftg_edges_df\n",
    "    .assign(\n",
    "        id_a=lambda df: df['name_a'].map(tftg_mapping),\n",
    "        id_b=lambda df: df['name_b'].map(tftg_mapping),\n",
    "    )\n",
    "    .dropna()\n",
    "    .assign(\n",
    "        id_a=lambda df: df['id_a'].astype(int),\n",
    "        id_b=lambda df: df['id_b'].astype(int),\n",
    "    )\n",
    "    .filter(items=['name_a', 'name_b', 'id_a', 'id_b', 'train', 'test_recon', 'test_new'])\n",
    ")\n",
    "\n",
    "tftg_edges_df.to_csv(data_path.joinpath('2.edges/tftg.tsv.xz'), compression='xz', index=False, sep='\\t')\n",
    "\n",
    "tftg_edges_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>id_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>train</th>\n",
       "      <th>test_recon</th>\n",
       "      <th>test_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AHR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AR</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name_a name_b  id_a  id_b  train  test_recon  test_new\n",
       "0    AHR    AHR     0     0      0           0         0\n",
       "1    AHR     AR     0     1      0           0         0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tftg_df = process_edges_to_full_network(tftg_edges_df, tftg_mapping, allow_loop=True, directed=True)\n",
    "\n",
    "tftg_df.to_csv(data_path.joinpath('3.all_nodes/tftg.tsv.xz'), compression='xz', index=False, sep='\\t')\n",
    "\n",
    "tftg_df.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xswap-analysis]",
   "language": "python",
   "name": "conda-env-xswap-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
