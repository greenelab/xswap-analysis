{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioRxiv database from Rxivist\n",
    "\n",
    "Rxivist full database (doi:10.5281/zenodo.2566421) at https://zenodo.org/record/2566421\n",
    "\n",
    "\n",
    "I used the following query to export a copy of the Rxivist BioRxiv scrape.\n",
    "\n",
    "```sqlite\n",
    "SELECT \n",
    "    aa.article, \n",
    "    art.title,\n",
    "    auth.id as author_id, \n",
    "    auth.name as author_name, \n",
    "    auth.institution, \n",
    "    art.doi, \n",
    "    art.collection, \n",
    "    art.posted\n",
    "FROM prod.article_authors aa\n",
    "JOIN prod.authors auth\n",
    "\tON aa.author = auth.id\n",
    "JOIN prod.articles art\n",
    "\tON aa.article = art.id\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import xswap\n",
    "\n",
    "import analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rxivist_df = pd.read_csv('../data/rxivist_result.csv')\n",
    "\n",
    "# Ensure that article:doi is a 1-1 mapping\n",
    "assert rxivist_df.groupby('article')['doi'].apply(lambda x: len(set(x))).max() == 1\n",
    "\n",
    "rxivist_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coauthor_df = (\n",
    "    rxivist_df\n",
    "    .drop(columns=['title', 'doi', 'institution'])\n",
    "    .merge(\n",
    "        rxivist_df\n",
    "        .drop(columns=['title', 'doi', 'institution']), \n",
    "        on=['article', 'posted', 'collection'], how='outer')\n",
    "    .query('author_id_x < author_id_y')\n",
    ")\n",
    "\n",
    "coauthor_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The first article shown has four authors. This should mean there are 6 coauthor relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rxivist_df.query('article == 2715').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coauthor_df.query('article == 2715')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bioinformatics collaboration network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature computation and permutation can become expensive when there are many (~ >10_000) edges\n",
    "# Therefore, we must pick a relatively early cutoff, 2016 and take only edges that exist <= 2016\n",
    "train_cutoff = 2016\n",
    "\n",
    "bioinformatics_df = (\n",
    "    coauthor_df\n",
    "    .query('collection == \"bioinformatics\"')\n",
    "    .assign(\n",
    "        year=lambda df: df['posted'].apply(lambda x: int(x[:4])),\n",
    "        test_new=1,\n",
    "        test_recon=lambda df: df['year'].apply(lambda x: int(x <= train_cutoff)),\n",
    "        train=lambda df: df['test_recon'].apply(lambda x: int(x and (np.random.rand() < 0.7)))\n",
    "    )\n",
    "    .drop_duplicates(subset=['author_id_x', 'author_id_y'])\n",
    "    .filter(items=['author_id_x', 'author_id_y', 'train', 'test_recon', 'test_new'])\n",
    ")\n",
    "\n",
    "bioinformatics_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = list(map(tuple, bioinformatics_df.query('train == 1')[['author_id_x', 'author_id_y']].values))\n",
    "\n",
    "# Map nodes to unique integers (for XSwap)\n",
    "mapped_edges, mapping, _ = xswap.preprocessing.map_str_edges(edges, bipartite=False)\n",
    "reversed_mapping = {v: k for k, v in mapping.items()}\n",
    "\n",
    "# Create a matrix of train edges (for feature computation)\n",
    "mat = analysis.edges_to_matrix(mapped_edges)\n",
    "\n",
    "# Create source, target degree matrices\n",
    "degree = np.repeat(mat.sum(axis=1), mat.shape[1], axis=1) \\\n",
    "       + np.repeat(mat.sum(axis=0), mat.shape[0], axis=0)\n",
    "\n",
    "# Use only those nodes that are present in the training network\n",
    "nodes = sorted(set(mapping.values()))\n",
    "source, target = zip(*itertools.product(nodes, nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.DataFrame()\n",
    "    .assign(\n",
    "        mapped_source=source,\n",
    "        mapped_target=target,\n",
    "        source=lambda df: df['mapped_source'].map(reversed_mapping),\n",
    "        target=lambda df: df['mapped_target'].map(reversed_mapping),\n",
    "    )\n",
    "    .merge(bioinformatics_df, left_on=['source', 'target'], right_on=['author_id_x', 'author_id_y'], how='left')\n",
    "    .drop(columns=['author_id_x', 'author_id_y'])\n",
    "    .fillna(0)\n",
    "    .assign(\n",
    "        source_degree=lambda df: df.groupby('mapped_source')['train'].sum(),\n",
    "        target_degree=lambda df: df.groupby('mapped_target')['train'].sum(),\n",
    "    )\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features on unpermuted network\n",
    "feature_mats = {\n",
    "    'prior_empirical': np.zeros(mat.shape),\n",
    "    \n",
    "    'rwr': analysis.invertible_rwr(mat, 0.25),\n",
    "    'mean_rwr': np.zeros(mat.shape),\n",
    "    'p_rwr': np.zeros(mat.shape),\n",
    "    \n",
    "    'jaccard': (mat@mat) / (degree - mat@mat),\n",
    "    'mean_jaccard': np.zeros(mat.shape),\n",
    "    'p_jaccard': np.zeros(mat.shape),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RWR p-value\n",
    "n_perms = 1000\n",
    "perm_edges = mapped_edges.copy()\n",
    "for i in tqdm.tnrange(n_perms):\n",
    "    # Permute edges\n",
    "    perm_edges, _ = xswap.permute_edge_list(perm_edges, allow_self_loops=False, \n",
    "                                            allow_antiparallel=False, seed=i)\n",
    "    perm_mat = analysis.edges_to_matrix(perm_edges)\n",
    "    feature_mats['prior_empirical'] += perm_mat\n",
    "    \n",
    "    # Compute RWR on permuted network\n",
    "    perm_rwr = analysis.invertible_rwr(perm_mat, 0.25)\n",
    "    feature_mats['mean_rwr'] += perm_rwr\n",
    "    feature_mats['p_rwr'] += (perm_rwr >= feature_mats['rwr'])\n",
    "    \n",
    "    # Compute Jaccard similarity on permuted network\n",
    "    A2 = perm_mat@perm_mat\n",
    "    perm_jac = A2 / (degree - A2)\n",
    "    feature_mats['mean_jaccard'] += perm_jac\n",
    "    feature_mats['p_jaccard'] += (perm_jac >= feature_mats['jaccard'])\n",
    "\n",
    "# Normalize features to number of permutations\n",
    "for feature in ['mean_rwr', 'p_rwr', 'mean_jaccard', 'p_jaccard', 'prior_empirical']:\n",
    "    feature_mats[feature] = feature_mats[feature] / n_perms\n",
    "    \n",
    "# Add computed features to the DataFrame\n",
    "for feature, values in feature_dict.items():\n",
    "    df[feature] = np.array(v).flatten()\n",
    "    \n",
    "df.to_csv('biorxiv_p_vs_rank.tsv.gz', compression='gzip', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xswap-analysis]",
   "language": "python",
   "name": "conda-env-xswap-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
